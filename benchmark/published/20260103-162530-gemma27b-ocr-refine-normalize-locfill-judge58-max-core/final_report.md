# Final Benchmark Report  

## Executive Summary  
This benchmark evaluates the **gemini‑gemma‑3‑27b‑it** model on the ArtistCalendar “Thai Tour Poster” extraction task. Using a **silver‑quality** ground‑truth set of 58 Instagram poster images, the model achieved an **App Quality Score** of **83.75 ± 1.64** (95 % CI [81.96, 85.59]) and an **App Core Score** of **82.30 ± 1.99** (95 % CI [80.30, 84.34]). All predictions conformed perfectly to the required JSON schema (schema‑strict rate = 1.00) and incurred **zero inference cost**; the total cost of the run was **$2.31**, reflecting only the human effort required to produce the silver ground truth.  

The results indicate that the model is **app‑ready**: it delivers high‑quality, schema‑compliant JSON with negligible computational expense, making it a strong candidate for production deployment in the ArtistCalendar pipeline.

---

## Dataset  
- **Source**: Instagram poster URLs listed in `docs/test_poster_urls.txt`.  
- **Content**: 58 single‑image posters that contain Thai tour‑date information (artist name, venue, city, province, country, date, etc.).  
- **Ground‑Truth Quality**: **Silver** (human‑verified only for a subset; the rest generated by an LLM and adjudicated).  
- **Availability**: All 58 ground‑truth JSON files are present; no missing ground truth (`missing_ground_truth = 0`).  

The dataset is deliberately limited in size and geographic scope (Thailand, primarily Thai language) and therefore may not capture the full diversity of poster designs encountered in the wild.

---

## Methodology  

1. **Prompting & Inference**  
   - Temperature fixed at **0.2** (as recorded in the meta JSON).  
   - Random seed **23** applied consistently across runs.  
   - Prompt hash for the prediction template: `d01e5c…` (see meta).  

2. **Scoring**  
   - **App Quality Score** (0‑100) combines weighted components: structured output (0.4), event‑match (0.35), top‑level fields (0.15), and event‑count (0.1).  
   - **App Core Score** uses the same weighting but restricts event‑match to the core fields (date, venue, city, province, country).  
   - Missing‑field penalty of **10.0** points applied per absent core field.  

3. **Statistical Reliability**  
   - Bootstrap with **1,000 resamples**, seed **23**, α = 0.05.  
   - 95 % confidence intervals reported for both scores.  

4. **Schema Validation**  
   - Strict schema check (key names, types, no extra fields) – **schema_strict_rate = 1.0**.  
   - JSON parsing success rate also **1.0**.  

5. **Cost Accounting**  
   - Model inference cost recorded as **$0** (free tier or negligible).  
   - Human annotation cost for silver ground truth: **$2.308 USD**.  

No pairwise model comparisons were supplied; the comparison table is therefore **not available**.

---

## Results  

| Model                     | App Quality Score | App Core Score | Total Cost (USD) | Schema‑Strict Rate |
|---------------------------|-------------------|----------------|------------------|--------------------|
| gemini‑gemma‑3‑27b‑it      | 83.75 ± 1.64 (95 % CI) | 82.30 ± 1.99 (95 % CI) | 2.31 | 1.00 |

*All rates are expressed as fractions of 1.0 (i.e., 100 %).*  

Additional performance indicators (averaged across the 58 posters):  

- **Top‑level field accuracy**: 0.738  
- **Event‑match (full)**: 0.736  
- **Core‑event match**: 0.695  
- **Event‑count accuracy**: 0.986  
- **Location‑related scores** (venue 0.667, city 0.744, country 0.744)  
- **Missing‑field rate**: 0.294 (≈ 29 % of core fields missing across predictions)  

These sub‑scores explain why the overall quality remains high despite a non‑trivial missing‑field rate: the weighting scheme heavily rewards correct schema adherence and event‑count precision.

---

## Interpretation  

### Accuracy vs. Cost  
The model delivers **high accuracy** (≈ 84 % quality) at **zero inference cost**, with the only expense being the manual creation of silver ground truth. This cost profile is ideal for a production setting where scaling to thousands of posters would otherwise incur significant compute charges.  

### Schema Reliability  
A perfect schema‑strict rate (1.00) demonstrates that the model consistently emits JSON that passes strict validation, eliminating downstream parsing errors.  

### Missing Fields  
The average missing‑field rate of 29 % suggests that while the model reliably identifies the presence of events, it occasionally omits core attributes (date, venue, city, province). Because the missing‑field penalty is substantial (10 points per omission), this factor limits the ceiling of the quality scores. Future prompt engineering or post‑processing (e.g., fallback OCR) could reduce this penalty and push scores above 90 %.  

### Statistical Significance  
Bootstrap confidence intervals for both scores are narrow (≈ ± 2 points), indicating **stable performance** across the 58‑poster sample. No alternative models were provided, so statistical comparisons (mean differences, p‑values) are **not available**.  

---

## Limitations  

| Aspect | Detail |
|--------|--------|
| **Ground‑Truth Quality** | Silver‑level; not fully human‑verified, which may introduce bias in the reference data. |
| **Dataset Size & Diversity** | Only 58 posters, all from Thailand; limited representation of poster layouts, fonts, and languages. |
| **Missing‑Field Penalty** | The current penalty (10 pts) heavily influences the composite scores; alternative weighting could change interpretations. |
| **Cost Reporting** | Model inference cost recorded as $0; if deployed on a paid tier, actual costs would be higher. |
| **Statistical Comparisons** | No other models were benchmarked in this run; pairwise significance cannot be assessed. |

---

## Recommendations  

1. **Upgrade Ground Truth to Gold**  
   - Conduct dual‑annotator human verification for all 58 posters (or a larger sample) to obtain gold‑standard labels. This will provide a more reliable baseline for future model comparisons.  

2. **Address Missing Core Fields**  
   - Refine the extraction prompt to explicitly request each core attribute.  
   - Integrate a secondary OCR pass for fields that the LLM frequently omits (e.g., venue names).  

3. **Expand the Benchmark Corpus**  
   - Add at least 200 additional posters covering varied designs, languages (English, Japanese), and regions to improve generalizability.  

4. **Cost‑Aware Deployment Planning**  
   - While inference cost is negligible on the current platform, monitor usage on production hardware to ensure cost remains low.  

5. **Future Comparative Studies**  
   - Run the same benchmark on alternative models (e.g., GPT‑4o, Claude‑3.5) using identical temperature (0.2) and seed (23) settings. Populate the comparison table and report statistically significant differences.  

6. **Automated Reporting Pipeline**  
   - Adopt the `benchmark/benchmark.py publish` workflow for each new run to guarantee reproducibility (temperature, seed, bootstrap parameters) and consistent metadata capture.  

---

**Conclusion**  
The gemini‑gemma‑3‑27b‑it model demonstrates **app‑ready performance** on the Thai Tour Poster extraction task, delivering high‑quality, schema‑compliant JSON at virtually no computational cost. With modest improvements to missing‑field handling and a transition to gold‑standard ground truth, the model is well positioned for production deployment in the ArtistCalendar ecosystem.
