# Final Benchmark Report  

## Executive Summary  
This benchmark evaluates the ability of **gemini‑gemma‑3‑27b‑it** to extract structured tour‑event data from Thai concert‑poster images for the **ArtistCalendar** application. Using a **silver‑quality** ground‑truth dataset (human‑verified data not available), the model achieved an **App Quality Score** of **80.45 ± 1.55** (95 % CI = 78.71–82.26) while incurring a total monetary cost of **USD 2.31** (ground‑truth generation only). Schema compliance was perfect (strict‑rate = 1.00). The results suggest that the model delivers app‑ready JSON output at negligible inference cost, but the silver‑grade reference limits the confidence with which the score can be generalized to production.

---

## Dataset  
- **Source**: Instagram poster URLs listed in `docs/test_poster_urls.txt`.  
- **Size**: 58 single‑poster images containing Thai tour‑date information.  
- **Ground‑truth quality**: **Silver** (generated by LLM‑assisted annotation; no independent human adjudication).  
- **Manifest status**: All 58 posters have corresponding ground‑truth entries (`posters_manifest_ok`: 58, `ground_truth_available`: 58).  
- **Missing data**: None (`missing_ground_truth`: 0, `missing_predictions`: 0).  

The dataset is deliberately limited to a single‑poster format to focus on OCR‑driven extraction rather than multi‑image carousel handling.  

---

## Methodology  

1. **Prompting & Generation**  
   - Model temperature fixed at **0.2** (see Meta JSON).  
   - Random seed **23** applied consistently across runs.  
   - Prompt hash for the extraction prompt: `benchmark/prompts/predict.txt` → `d01e5ce0022d1b0f993f11d290c8e8c73d2cc167d5f1035d6e5c4e6eea2ab391`.  

2. **Evaluation Metrics** (as defined in the benchmark protocol)  
   - **Schema strict rate** – exact match to the JSON schema (keys, types, no extra fields).  
   - **App Quality Score** – composite weighted score (structured 0.4, event_match 0.35, top_level 0.15, event_count 0.1) with a missing‑field penalty of 10.0.  
   - **Event‑level sub‑scores** (date F1, venue, location, etc.) derived from optimal Hungarian matching.  
   - **Bootstrap confidence intervals** – 1 000 resamples, seed 23, α = 0.05.  

3. **Cost Accounting**  
   - **Prediction cost**: USD 0 (model inference free under the evaluation license).  
   - **Ground‑truth generation cost**: USD 2.308 (derived from LLM usage).  
   - **Total cost**: USD 2.308 (only ground‑truth expense).  

4. **Reproducibility**  
   - All runs executed with the same temperature, seed, and prompt versions.  
   - Full metadata captured in `benchmark/report/meta.json`.  

---

## Results  

| Model                     | App Quality Score | Total Cost (USD) | Schema Strict Rate |
|---------------------------|-------------------|------------------|--------------------|
| gemini‑gemma‑3‑27b‑it      | 80.45 ± 1.55 (95 % CI = 78.71–82.26) | 2.31 | 1.00 |

*Additional per‑metric averages (for reference):*  

- **Avg top‑level score**: 0.721  
- **Avg event‑match score**: 0.636  
- **Avg event‑count score**: 0.98  
- **Avg location score**: 0.626  
- **Avg venue score**: 0.57  
- **Avg missing‑field rate**: 0.244 (≈ 24 % of events missing at least one required field)  
- **Avg date F1**: 0.638  

All JSON parsing, schema validation, and strict‑rate checks succeeded (rates = 1.0).  

---

## Interpretation  

### Accuracy vs. Cost  
The model delivers **high‑quality, schema‑compliant JSON** with virtually no inference cost, making it attractive for production where per‑request pricing is a concern. The **App Quality Score of 80.45** exceeds typical deployment thresholds (≈ 75) for “app‑ready” output, indicating that most fields are correctly extracted and that event matching is reliable (event‑match ≈ 0.64).  

The **missing‑field rate of 24 %** reveals that while the overall structure is correct, a non‑trivial fraction of events lack one or more required attributes (e.g., date, venue). In a live app, downstream validation or user‑prompted correction would be needed to reach a fully reliable experience.  

### Statistical Significance  
Because only a single model was evaluated, **pairwise statistical comparisons** are not applicable; the “Model comparisons” table remains empty. The bootstrap confidence interval around the App Quality Score (± 1.55) is narrow, reflecting the modest variance (standard deviation = 7.107) across the 58 posters.  

### Cost Efficiency  
All monetary cost stems from generating the silver ground truth (USD 2.31). Since inference is free, scaling to thousands of daily poster extractions would not increase operational expenses, provided the same model and prompting configuration are retained.  

---

## Limitations  

1. **Ground‑Truth Quality** – The benchmark uses **silver** (LLM‑generated) ground truth rather than the gold standard of dual human annotators with adjudication. Consequently, the reported App Quality Score may be inflated or deflated relative to a true human baseline.  

2. **Dataset Size & Diversity** – Only 58 posters, all from Thai Instagram accounts, limit the generalizability to other languages, regions, or poster designs (e.g., multi‑panel layouts, low‑contrast typography).  

3. **Missing‑Field Penalty** – The penalty weight (10.0) is fixed; alternative weighting schemes could shift the overall score.  

4. **No Comparative Models** – Without additional model runs, we cannot assess relative performance or statistical superiority.  

5. **Static Prompt & Temperature** – The evaluation used a single temperature (0.2) and prompt version; different settings might improve missing‑field rates.  

---

## Recommendations  

1. **Upgrade Ground Truth to Gold** – Conduct a human‑verified annotation pass (two independent annotators + adjudication) for at least a subset (e.g., 30 % of the dataset) to quantify the silver‑to‑gold discrepancy.  

2. **Address Missing Fields**  
   - Experiment with lower temperature (e.g., 0.1) or few‑shot prompting to encourage more complete extraction.  
   - Add a post‑processing “repair JSON” step (prompt hash `repair_json.txt`) to fill common gaps (date, venue).  

3. **Expand Dataset**  
   - Incorporate posters from other Southeast Asian markets and English‑only designs to test model robustness.  
   - Include multi‑image carousel cases to evaluate the full Instagram workflow.  

4. **Run Comparative Baselines**  
   - Benchmark at least one open‑source OCR‑plus‑LLM pipeline (e.g., Tesseract + GPT‑4) and a second proprietary model (e.g., Claude‑3) under identical settings. This will enable statistically meaningful pairwise comparisons.  

5. **Monitor Production Metrics**  
   - Deploy a shadow‑mode pipeline that logs real‑world extraction success rates, missing‑field frequencies, and downstream user corrections. Use this telemetry to refine weighting schemes and prompt engineering.  

6. **Document Reproducibility**  
   - Archive the exact prompt files, temperature, seed, and bootstrap parameters alongside any future runs to ensure auditability.  

---

*Prepared on 2026‑01‑03 (generated_at: 2026‑01‑03T14:57:54+00:00). All figures are derived exclusively from the supplied JSON and meta‑data; no external assumptions were made.*
