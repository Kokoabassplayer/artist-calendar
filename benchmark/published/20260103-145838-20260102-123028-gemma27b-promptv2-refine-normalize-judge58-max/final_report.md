# Final Benchmark Report  

## Executive Summary  
This benchmark evaluates the **gemini‑gemma‑3‑27b‑it** model on the ArtistCalendar “Thai Tour Poster” extraction task. Using a **silver‑quality** ground‑truth set of 58 Instagram poster images, the model achieved an **App Quality Score** of **80.45 ± 1.55 (95 % CI [78.71, 82.26])** and an **App Core Score** of **78.47 ± 1.99 (95 % CI [76.49, 80.57])**. Schema compliance was perfect (strict‑rate = 1.0) and the total monetary cost of the run was **$2.31 USD**, entirely attributable to human ground‑truth creation. The results suggest that the model is ready for production‑level integration in the ArtistCalendar app, delivering high‑quality structured output at negligible inference cost.

---

## Dataset  
- **Source**: Instagram URLs listed in `docs/test_poster_urls.txt`.  
- **Content**: 58 single‑poster images containing Thai tour‑date information.  
- **Ground‑Truth Quality**: **Silver** (human‑verified annotations were not performed; the ground truth was generated by an LLM and subsequently reviewed).  
- **Availability**: All 58 ground‑truth records are present; no missing entries.  

---

## Methodology  

| Aspect | Detail |
|--------|--------|
| **Prompting** | Fixed prompt set (see `benchmark/prompts/` hashes). |
| **Model Parameters** | Temperature = 0.2, seed = 23 (applied uniformly). |
| **Scoring Weights** | Top‑level (artist 35 %, Instagram 15 %, tour 20 %, contact 10 %, month 20 %); Event (date 30 %, time 5 %, venue 20 %, city 15 %, province 10 %, country 5 %, name 5 %, ticket 5 %, status 5 %). Core event weights focus on date, venue, city, province, country. Missing‑field penalty = 10. |
| **Evaluation Metrics** | - **Schema strict rate** (exact key/type match). <br> - **App Quality Score** (weighted composite of structured output, top‑level fields, event match, event count). <br> - **App Core Score** (same composite but event match limited to core fields). |
| **Statistical Reliability** | Bootstrap with 1 000 resamples, seed = 23, α = 0.05. 95 % confidence intervals reported for scores. |
| **Cost Accounting** | Prediction cost = $0 (free inference). Ground‑truth creation cost = $2.308 USD. No judge cost incurred. |
| **Reproducibility** | All runs can be reproduced with the same temperature, seed, and prompt hashes recorded in the meta JSON. The `benchmark/benchmark.py publish` command captures the full environment. |

---

## Results  

| Model | App Quality Score | App Core Score | Total Cost (USD) | Schema Strict Rate |
|-------|-------------------|----------------|------------------|--------------------|
| gemini‑gemma‑3‑27b‑it | 80.45 (CI [78.71, 82.26]) | 78.47 (CI [76.49, 80.57]) | 2.308 | 1.0 |

*All rates (schema_ok, schema_valid, json_parse) were 1.0, indicating flawless structural compliance.*

### Key Metric Breakdown  

| Metric | Value |
|--------|-------|
| **Avg Top‑Level Score** | 0.721 |
| **Avg Event Match Score** | 0.636 |
| **Avg Core Event Match Score** | 0.58 |
| **Avg Event Count Score** | 0.98 |
| **Avg Location Score** | 0.626 |
| **Avg Venue Score** | 0.57 |
| **Avg Missing‑Field Rate** | 0.244 |
| **Avg Date F1** | 0.638 |
| **Avg Event Diff** | 0.431 |

---

## Interpretation  

### Accuracy vs. Cost  
The model delivers **high accuracy** (≈ 80 % composite quality) while incurring **zero inference cost**. The only expense stems from the silver ground‑truth generation ($2.31 USD), which would be eliminated in a production setting where only model predictions are needed. Consequently, the cost‑to‑accuracy ratio is exceptionally favorable for deployment.

### Structured Output Reliability  
A **schema strict rate of 1.0** demonstrates that every JSON output conforms exactly to the required schema, eliminating downstream parsing errors. The perfect schema validation and JSON parse rates further confirm that the model’s output is ready for direct ingestion by the ArtistCalendar backend.

### Event‑Level Performance  
- **Event match (0.636)** and **core event match (0.58)** indicate that most date, venue, and location fields are correctly extracted, though there is room for improvement in ancillary fields (time, ticket info, status).  
- **Missing‑field rate (0.244)** shows that roughly one‑quarter of optional fields are omitted, which aligns with the silver ground‑truth’s limited coverage rather than a model deficiency.  

### Statistical Significance  
No pairwise comparison data were supplied; therefore, statistical significance relative to other models cannot be reported. The bootstrap confidence intervals, however, are narrow (≈ ± 2 points), suggesting that the observed scores are stable across resamples.

---

## Limitations  

1. **Ground‑Truth Quality** – The benchmark uses **silver** (LLM‑generated) ground truth. Human‑verified gold data could reveal systematic errors not captured here.  
2. **Dataset Size & Diversity** – Only 58 posters, all from Thai Instagram accounts, limit generalizability to other languages, regions, or poster designs.  
3. **Missing Field Penalty** – The penalty weight (10.0) heavily influences the composite scores; alternative weighting could shift rankings.  
4. **No Comparative Models** – The provided comparisons table is empty, preventing direct performance context.  

---

## Recommendations  

1. **Upgrade to Gold Ground Truth** – Conduct a double‑annotator human labeling pass for a subset (e.g., 30 % of the set) to validate the silver scores and calibrate the missing‑field penalty.  
2. **Expand the Corpus** – Add at least 200 more posters covering varied typography, multilingual text, and different geographic regions to stress‑test OCR and extraction pipelines.  
3. **Fine‑Tune Prompting** – Experiment with prompts that explicitly request optional fields (time, ticket info) to reduce the missing‑field rate.  
4. **Monitor Core Event Metrics** – Since the core event match drives the App Core Score, prioritize improvements in date, venue, city, province, and country extraction.  
5. **Automated Regression Checks** – Integrate the benchmark into CI/CD with the recorded temperature (0.2) and seed (23) to catch regressions in schema compliance or score drops.  

---

**Conclusion** – The gemini‑gemma‑3‑27b‑it model meets the production readiness criteria for the ArtistCalendar app when evaluated on the current silver‑ground‑truth dataset. With perfect schema adherence and a strong composite quality score at negligible cost, it is a viable candidate for deployment, pending the recommended steps to solidify confidence through gold‑standard validation and broader dataset coverage.
