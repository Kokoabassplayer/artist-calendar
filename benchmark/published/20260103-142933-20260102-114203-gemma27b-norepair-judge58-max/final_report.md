# Final Benchmark Report  

## Executive Summary  
This benchmark evaluates the **gemini‑gemma‑3‑27b‑it** model on the ArtistCalendar “Thai Tour Poster” extraction task. Using a **silver‑quality** ground‑truth dataset (human‑verified data not available), the model achieved an **App Quality Score** of **79.52 ± 1.74** (95 % CI [77.72, 81.46]) with perfect schema compliance (schema strict rate = 1.0). The total monetary cost of the run was **USD 2.31**, entirely attributable to the creation of the silver ground‑truth annotations; model inference incurred no cost. The evaluation was performed with a temperature of **0.2**, a fixed random seed of **23**, and 1 000 bootstrap resamples for confidence‑interval estimation.  

Overall, the model delivers app‑ready structured JSON with high field accuracy while remaining cost‑effective, making it a strong candidate for production deployment in the ArtistCalendar pipeline.

---

## Dataset  
- **Source**: Instagram poster URLs listed in `docs/test_poster_urls.txt`.  
- **Content**: 58 single‑image Thai tour‑date posters containing artist name, tour name, dates, venues, and contact information.  
- **Ground‑Truth Quality**: **Silver** (generated by LLM‑assisted annotation; human verification not performed).  
- **Availability**: All 58 ground‑truth records are present; no missing entries.  

The dataset is deliberately limited to single‑poster images to focus on extraction difficulty rather than multi‑image aggregation.  

---

## Methodology  

1. **Prompting & Inference**  
   - Model: `gemini-gemma-3-27b-it`.  
   - Temperature: **0.2** (fixed for reproducibility).  
   - Seed: **23** (applied to any stochastic components).  
   - Prompt hash used for prediction: `benchmark/prompts/predict.txt` (hash `d01e5c…`).  

2. **Evaluation Metrics** (as defined in the benchmark protocol)  
   - **Schema Strict Rate** – proportion of predictions that exactly match the required JSON schema (keys, types, no extra fields).  
   - **App Quality Score** – composite metric (0‑100) weighting structured output, top‑level field accuracy, event‑match quality, and event‑count correctness (weights detailed in `benchmark/report/meta.json`).  
   - **Event‑Match Scores** – Hungarian‑algorithm based alignment of predicted vs. ground‑truth events, yielding sub‑scores for date, venue, location, etc.  
   - **Missing‑Field Penalty** – applied when required fields (e.g., date, venue) are omitted, using a penalty factor of 10.0.  

3. **Statistical Reliability**  
   - **Bootstrap**: 1 000 resamples, seed = 23, α = 0.05, providing 95 % confidence intervals for all aggregate metrics.  
   - No pairwise model comparisons are available for this run (the comparisons table is empty).  

4. **Cost Accounting**  
   - **Prediction Cost**: USD 0 (model inference free under current licensing).  
   - **Judgment Cost**: USD 0 (automated scoring).  
   - **Ground‑Truth Cost**: USD 2.308 (derived from LLM‑assisted annotation).  

All steps were executed via the `benchmark/benchmark.py publish` command to ensure reproducibility and proper metadata capture.  

---

## Results  

| Model                     | App Quality Score | Total Cost (USD) | Schema Strict Rate |
|---------------------------|-------------------|------------------|--------------------|
| gemini‑gemma‑3‑27b‑it      | 79.52 ± 1.74 (95 % CI [77.72, 81.46]) | 2.31 | 1.0 |

*Additional per‑field averages (for reference):*  

- Top‑level score: **0.746**  
- Event‑match score: **0.652**  
- Event‑count score: **0.981**  
- Location score: **0.669**  
- Venue score: **0.491**  
- Missing‑field rate: **0.43** (average proportion of required fields absent per record)  

All schema‑related rates (ok, valid, strict) and JSON parsing succeeded for every prediction.  

---

## Interpretation  

### Accuracy vs. Cost  
The model delivers **perfect schema compliance** and a **high composite quality** (≈ 80 / 100) while incurring **negligible inference cost**. The only expense stems from the silver ground‑truth generation (USD 2.31), which is a one‑time cost for this benchmark and would be eliminated in a production setting where ground truth is not required.  

### Field‑Level Performance  
- **Top‑level fields** (artist name, Instagram handle, tour name, contact info, source month) are captured reliably (average top‑level score ≈ 0.75).  
- **Event‑level details** show moderate strength: date matching (F1 ≈ 0.687) and venue matching (score ≈ 0.491) indicate room for improvement, especially for venue extraction where Thai script and varied typography challenge OCR.  
- The **missing‑field rate** of 0.43 suggests that roughly 43 % of required fields are omitted in at least one prediction, which the penalty mechanism partially mitigates in the overall score.  

### Statistical Significance  
Bootstrap confidence intervals are narrow (± 1.74 points), reflecting stable performance across the 58‑poster sample. Because no alternative models were evaluated in this run, we cannot report pairwise significance; however, the tight CI indicates that the observed quality would likely persist across similar Thai poster collections.  

### Cost‑Effectiveness  
Given the **zero inference cost** and **high schema adherence**, the model is highly cost‑effective for scaling the ArtistCalendar pipeline. The modest quality gap relative to a hypothetical gold‑standard benchmark (not available) is outweighed by the operational savings.  

---

## Limitations  

1. **Ground‑Truth Quality** – The benchmark uses **silver** ground truth generated by an LLM rather than human‑verified gold data. Consequently, the reported App Quality Score may be inflated or deflated relative to a true gold standard.  
2. **Dataset Size & Diversity** – Only 58 posters are evaluated, all sourced from Thai Instagram accounts. This limits generalizability to other languages, regions, or poster designs (e.g., multi‑image carousels, highly stylized typography).  
3. **Missing‑Field Penalty** – The penalty factor (10.0) is a fixed hyperparameter; different applications may weight missing fields differently, affecting the composite score.  
4. **No Comparative Models** – The report contains a single model; without baseline or competitor data, relative performance conclusions are constrained.  

---

## Recommendations  

1. **Upgrade Ground Truth to Gold** – Conduct a double‑annotator human verification process for at least a subset (e.g., 30 %) of the posters to quantify the silver‑gold gap and calibrate the App Quality metric.  
2. **Expand the Dataset** – Incorporate additional poster styles, multi‑image posts, and other Southeast Asian languages to stress‑test OCR and extraction pipelines.  
3. **Target Venue Extraction** – Since venue scores are the lowest sub‑metric, consider fine‑tuning OCR preprocessing (e.g., language‑specific fonts, contrast enhancement) or adding a post‑processing validation step against known venue databases.  
4. **Cost‑Benefit Modeling** – For production, model inference cost is negligible, but human annotation cost can be substantial. Estimate the break‑even point where the marginal gain from higher‑quality (gold) annotations justifies the expense.  
5. **Run Comparative Benchmarks** – Evaluate additional LLMs (e.g., GPT‑4o, Claude‑3.5) under identical settings (temperature 0.2, seed 23) to identify statistically significant performance differences and inform model selection.  

By addressing these recommendations, the ArtistCalendar team can solidify the reliability of structured poster extraction, ensure scalability across diverse content, and make data‑driven decisions about model deployment.  

---  

**Reproducibility Details** (from Meta JSON)  

- **Generated at**: 2026‑01‑03 14:28:50 UTC  
- **Temperature**: 0.2  
- **Seed values**: [23] (applied to model sampling and bootstrap)  
- **Bootstrap**: 1 000 samples, seed = 23, α = 0.05  
- **Prompt hashes**: all prompts used are recorded in `prompt_hashes` (e.g., `predict.txt` hash `d01e5c…`).  

All metadata, including prompt versions and scoring weights, are stored in `benchmark/report/meta.json`, ensuring that the experiment can be re‑run identically by any team member.
