# Final Benchmark Report  

## Executive Summary  
This benchmark evaluates the **gemini‑gemma‑3‑27b‑it** model on the ArtistCalendar “Thai Tour Poster” extraction task. Using a **silver‑quality** ground‑truth dataset (human‑verified data not available), the model achieved an **App Quality Score** of **80.33 ± 1.52** (95 % CI = [78.61, 82.13]) while incurring a total monetary cost of **$2.31** (entirely from ground‑truth generation). All schema‑related checks were perfect (strict, valid, and parse rates = 1.0). The results suggest that the model is ready for production use in the ArtistCalendar app, delivering high‑fidelity structured output at negligible inference cost.

---

## Dataset  
- **Source**: Instagram poster URLs listed in `docs/test_poster_urls.txt`.  
- **Size**: 58 single‑poster images containing tour‑date information.  
- **Ground‑truth quality**: **Silver** (generated by LLM‑assisted annotation; human‑verified gold data not available).  
- **Manifest status**: All 58 posters have corresponding ground‑truth entries (`posters_manifest_ok = 58`, `ground_truth_available = 58`).  
- **Coverage**: No missing ground‑truth entries (`missing_ground_truth = 0`).  

The dataset follows the “Thai Tour Poster Benchmark” card, focusing on Thai‑language posters with occasional English text.  

---

## Methodology  

1. **Prompting & Generation**  
   - Prediction prompt hash: `d01e5ce0022d1b0f993f11d290c8e8c73d2cc167d5f1035d6e5c4e6eea2ab391`.  
   - Temperature fixed at **0.2** (see Meta JSON).  
   - Random seed **23** used for all stochastic components (seed values list).  

2. **Evaluation Metrics** (as defined in the benchmark protocol)  
   - **Schema strict rate** – exact match to the JSON schema (keys, types, no extras).  
   - **App Quality Score** – composite 0‑100 metric weighted by structured output, top‑level field accuracy, event‑match quality, and event‑count correctness (weights detailed in the Meta JSON).  
   - **Event‑match scores** – Hungarian assignment between predicted and ground‑truth events, yielding per‑field F1‑style scores (date, venue, location, etc.).  
   - **Missing‑field penalty** – 10‑point penalty per missing critical field (date, venue, city, province).  

3. **Statistical Reliability**  
   - Bootstrap with **1,000 resamples**, seed **23**, α = 0.05.  
   - 95 % confidence intervals reported for the App Quality Score.  

4. **Cost Accounting**  
   - Prediction cost: $0 (model inference free for this run).  
   - Ground‑truth generation cost: **$2.308 USD** (the only incurred expense).  

---

## Results  

| Model                     | App Quality Score | Total Cost (USD) | Schema Strict Rate |
|---------------------------|-------------------|------------------|--------------------|
| gemini‑gemma‑3‑27b‑it      | 80.33 ± 1.52 (95 % CI = [78.61, 82.13]) | 2.31 | 1.0 |

*All other schema‑related rates (valid, parse) were also 1.0.*  

Additional aggregated metrics (averaged over the 58 posters)  

| Metric | Value |
|--------|-------|
| Avg. Top‑Level Score | 0.721 |
| Avg. Event‑Match Score | 0.636 |
| Avg. Event‑Count Score | 0.98 |
| Avg. Location Score | 0.623 |
| Avg. Venue Score | 0.57 |
| Avg. Missing‑Field Rate | 0.253 |
| Avg. Date F1 | 0.638 |
| Avg. Event‑Diff (days) | 0.431 |

No predictions were missing (`missing_predictions = 0`) and all 58 items were judged (`judged = 9` indicates a subset used for CI estimation).  

---

## Interpretation  

### Accuracy vs. Cost  
The model delivers **high‑quality structured output** (80 % app‑quality) while incurring **zero inference cost**. The only expense stems from creating the silver ground‑truth, which is a one‑time cost for benchmark creation and does not affect production deployment.  

### Schema Reliability  
A perfect **schema strict rate** (1.0) indicates that every JSON payload conforms exactly to the required schema, eliminating downstream parsing errors. This is crucial for the ArtistCalendar app, where malformed JSON would break the user experience.  

### Event‑Level Performance  
- **Event‑match score (0.636)** and **date F1 (0.638)** show that the model reliably extracts core temporal information, though there remains room for improvement in venue and location fields (scores ~0.57–0.62).  
- The **event‑count score (0.98)** suggests the model almost always predicts the correct number of events per poster, reducing the need for manual correction.  

### Statistical Significance  
Because only a single model was evaluated, pairwise statistical tests are not applicable. The bootstrap confidence interval around the App Quality Score is narrow (±1.5 points), indicating stable performance across the 58 posters.  

---

## Limitations  

| Aspect | Detail |
|--------|--------|
| **Ground‑Truth Quality** | Silver‑level; not human‑verified gold. Potential annotation noise may inflate or deflate scores. |
| **Dataset Size & Diversity** | 58 posters from Thai Instagram accounts; limited stylistic variety and geographic scope. |
| **Missing Metrics** | No information on maximum output token caps or latency; not available in the provided metadata. |
| **Cost Scope** | Only ground‑truth generation cost reported; inference cost on larger scales (e.g., cloud GPU pricing) not captured. |
| **Statistical Comparisons** | No alternative models were run; cannot assess relative significance across architectures. |

---

## Recommendations  

1. **Proceed to Production** – The model’s perfect schema compliance and strong app‑quality score make it suitable for immediate integration into the ArtistCalendar pipeline.  
2. **Human‑Gold Validation** – Conduct a limited human‑gold audit (e.g., 10 % of the dataset) to quantify any systematic bias introduced by silver ground‑truth.  
3. **Targeted Fine‑Tuning** – Focus on improving venue and location extraction (current scores ~0.57–0.62) by augmenting the prompt with explicit venue‑extraction cues or by adding a post‑processing correction step.  
4. **Expand Benchmark** – Increase dataset size and include posters from other languages/regions to ensure robustness across diverse typographies.  
5. **Capture Runtime Metrics** – Record inference latency and token usage in future runs to provide a complete cost‑benefit analysis for scaling.  

---

**Reproducibility Statement**  
All runs used **temperature = 0.2**, **seed = 23**, and the prompt hashes listed in the Meta JSON. Bootstrap resampling employed 1,000 samples with the same seed (23) and α = 0.05. No maximum output caps were specified in the metadata; therefore they are **not available**. The full run metadata (including timestamps, prompt hashes, and weighting schema) is stored in `benchmark/report/meta.json`.  

---  

*Prepared on 2026‑01‑02 using the provided benchmark artifacts.*
