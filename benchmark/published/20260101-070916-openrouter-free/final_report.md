# Final Benchmark Report  

## Executive Summary  
This benchmark evaluates two open‑router vision‑language models on the **ArtistCalendar** poster‑extraction task using a curated set of 58 Thai tour‑date posters. The goal is to produce **app‑ready, schema‑strict JSON** that can be ingested directly by the ArtistCalendar mobile application.  

* **Ground‑truth quality:** **silver** (LLM‑generated, not human‑verified).  
* **Key findings:**  
  * Both models achieve high schema compliance (strict rate ≈ 0.71–0.78) and low prediction cost (USD 0.0).  
  * The Qwen‑2.5‑VL‑7B model attains a marginally higher **app quality score** (70.9 ± 17.5) than the Nemotron‑Nano‑12B model (68.0 ± 22.1), but the bootstrap comparison shows the difference (‑2.89 points) is **not statistically significant** (p = 0.43).  
  * Event‑level metrics (date F1, event count accuracy) are consistently better for Qwen‑2.5, indicating more reliable extraction of individual tour events.  

Overall, both models are viable candidates for production, with Qwen‑2.5 offering a modest accuracy edge at comparable cost.

---

## Dataset  
* **Source:** Instagram poster URLs listed in `docs/test_poster_urls.txt`.  
* **Size:** 58 single‑poster images containing Thai tour‑date information.  
* **Ground‑truth:** Silver‑level JSON generated by an LLM (see Protocol “Ground truth (gold)”). No human adjudication was performed.  
* **Manifest status:** All 58 posters are present (`posters_manifest_ok: 58`).  

The dataset is deliberately limited to Thai‑language posters, which introduces a geographic and linguistic bias (see Limitations).

---

## Methodology  

1. **Prompting & Generation**  
   * Temperature fixed at **0.1** for all runs (Meta JSON).  
   * Random seed **23** applied uniformly (seed_values: [23]).  
   * Prompt hashes for `ground_truth.txt`, `interpret.txt`, `judge.txt`, and `predict.txt` are recorded in the meta file to guarantee reproducibility.  

2. **Evaluation Pipeline**  
   * Each model generated JSON predictions for all posters. Missing predictions were counted (3 for Nemotron‑Nano, 1 for Qwen‑2.5).  
   * Predictions were parsed (`json_parse_rate`) and validated against the strict schema (`schema_strict_rate`).  
   * **App quality score** (0‑100) combines weighted components: structured output, top‑level field accuracy, event‑match quality, and event‑count correctness (weights defined in the Meta JSON).  
   * Event‑level matching employed a Hungarian assignment to compute `avg_event_match_score`, `avg_event_count_score`, `avg_date_f1`, etc.  

3. **Statistical Reliability**  
   * Bootstrap resampling: 1,000 samples, seed 23, α = 0.05 (see Meta JSON).  
   * Pairwise mean differences, 95 % confidence intervals, and p‑values are reported in the comparisons table.  

4. **Cost Accounting**  
   * Prediction and judging costs were zero (free model endpoints).  
   * Ground‑truth generation cost: **USD 2.308 029** (same for both runs).  
   * Total cost per run equals the ground‑truth cost (no additional inference expense).  

---

## Results  

| Model | App Quality Score | Total Cost (USD) | Schema Strict Rate |
|-------|-------------------|------------------|--------------------|
| openrouter‑nvidia_nemotron‑nano‑12b‑v2‑vl_free | 68.01 (95 % CI 62.07–73.44) | 2.308 029 | 0.776 |
| openrouter‑qwen_qwen‑2.5‑vl‑7b‑instruct_free | 70.90 (95 % CI 66.29–75.44) | 2.308 029 | 0.707 |

**Additional performance highlights**

| Metric | Nemotron‑Nano | Qwen‑2.5 |
|--------|---------------|----------|
| `avg_top_level_score` | 0.75 | 0.774 |
| `avg_event_match_score` | 0.573 | 0.635 |
| `avg_event_count_score` | 0.856 | 0.958 |
| `avg_location_score` | 0.555 | 0.656 |
| `avg_venue_score` | 0.402 | 0.447 |
| `avg_missing_field_rate` | 0.547 | 0.428 |
| `avg_event_diff` (events per poster) | 2.741 | 0.879 |
| `avg_date_f1` | 0.681 | 0.746 |
| `json_parse_rate` | 0.948 | 0.983 |

Both models parse >94 % of predictions successfully, with Qwen‑2.5 achieving near‑perfect parsing. The lower `avg_missing_field_rate` and `avg_event_diff` for Qwen‑2.5 indicate fewer omitted or spurious events.

---

## Interpretation  

### Accuracy vs. Cost  
The cost dimension is identical for both models (USD 2.308 029) because inference is free; therefore, the trade‑off reduces to **accuracy and reliability**. Qwen‑2.5 delivers a modest 2.9‑point advantage in app quality and consistently higher event‑level scores, suggesting better suitability for downstream scheduling features (e.g., reminders, venue mapping).  

### Statistical Significance  
Bootstrap analysis yields a mean difference of **‑2.888** (Nemotron – Qwen) with a 95 % CI of **‑10.53 to 3.598** and a p‑value of **0.43**. The interval crosses zero and the p‑value exceeds the conventional 0.05 threshold, indicating **no statistically significant superiority** of either model under the current silver‑ground‑truth regime.  

### Structured Output Reliability  
Both models meet the strict schema requirement (>70 % strict rate). However, the **schema_valid_rate** (allowing extra fields) is higher for Nemotron‑Nano (0.776) than for Qwen‑2.5 (0.707), implying that Qwen‑2.5 occasionally adds non‑schema fields that are later filtered out. The high `json_parse_rate` for Qwen‑2.5 reduces the risk of downstream parsing failures.  

### Impact of Silver Ground Truth  
Because the reference annotations are LLM‑generated (silver), the absolute quality scores may be inflated or deflated relative to a human‑verified gold set. Consequently, the observed differences should be interpreted as **relative performance** rather than definitive absolute accuracy.

---

## Limitations  

| Aspect | Detail |
|--------|--------|
| **Ground‑truth quality** | Silver‑level only; no human adjudication, which may introduce systematic bias. |
| **Dataset size & diversity** | 58 posters, all Thai‑language, limited typographic variety; results may not generalize to other languages or poster styles. |
| **Missing data** | Not available: maximum output token caps, latency measurements, and hardware specifications. |
| **Evaluation scope** | Focuses on structured JSON extraction; does not assess downstream app behavior (e.g., UI rendering, user satisfaction). |
| **Statistical power** | Small sample size limits detection of modest performance gaps; bootstrap CIs are relatively wide. |

---

## Recommendations  

1. **Upgrade Ground Truth to Gold**  
   * Conduct dual human annotation with adjudication to obtain gold‑standard JSON. This will enable more reliable absolute quality assessments and better inform production decisions.  

2. **Model Selection**  
   * For a production pipeline where inference cost is negligible, **Qwen‑2.5‑VL‑7B** is the preferred choice due to its higher event‑match scores and lower missing‑field rate, despite the lack of statistical significance.  

3. **Schema Enforcement**  
   * Implement a post‑processing step that strips any non‑schema fields (observed more frequently in Qwen‑2.5) before ingestion into the app.  

4. **Expand Dataset**  
   * Augment the benchmark with additional languages (e.g., English, Japanese) and varied poster designs to improve external validity.  

5. **Monitor Runtime Metrics**  
   * Capture latency, GPU memory usage, and token limits in future runs to assess operational feasibility at scale.  

6. **Reproducibility Checklist**  
   * Preserve temperature (0.1), seed (23), prompt hashes, and bootstrap parameters (samples = 1000, seed = 23, α = 0.05) in all published runs.  
   * Document any changes to model endpoints or API versions to maintain traceability.  

By addressing the ground‑truth limitation and broadening the evaluation corpus, the ArtistCalendar team can confidently select a model that balances **high‑fidelity structured extraction** with **operational efficiency** for real‑world deployment.
