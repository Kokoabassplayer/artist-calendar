# Final Benchmark Report

## Executive Summary

This report presents a benchmark evaluation of several language models for extracting structured data from Thai tour posters. The goal is to assess their suitability for the ArtistCalendar application, focusing on the quality and reliability of the structured output. The ground truth used in this evaluation is **silver** data, meaning it was generated by an LLM and not verified by human annotators. This limits the conclusions that can be drawn about absolute accuracy, but still allows for relative comparisons between models. The models were evaluated on a dataset of 58 posters, with varying numbers of posters judged for each model. The primary metric is the "app_quality_score," a composite score reflecting the accuracy of extracted information and adherence to the defined schema.

## Dataset

The dataset consists of 58 Thai tour poster images sourced from Instagram, as defined by the URLs in `docs/test_poster_urls.txt`. The dataset includes single-poster images containing tour-date information. Multi-image carousels, non-tour announcements, and posters without dates are excluded. The dataset is versioned using the SHA-256 hash of the URL list and manifest. The dataset is geographically biased toward Thailand and Thai-language content. Poster styles and typography vary, leading to uneven OCR difficulty.

## Methodology

Each model was used to extract structured data from the poster images, following the schema defined in `benchmark/prompts/ground_truth.txt`. The models were run with a temperature of 0.1 and a fixed random seed of 23. The extracted data was then compared to the silver ground truth, and various metrics were calculated, including schema strict rate, app quality score, and average scores for different fields. The app quality score is a weighted composite of structured output, top-level fields, event matching, and event count. Statistical significance was assessed using bootstrap confidence intervals (1,000 resamples, seed 23, alpha 0.05) and pairwise comparisons.

## Results

| Model                                                    | app_quality_score | total_cost_usd | schema_strict_rate |
| -------------------------------------------------------- | ----------------- | -------------- | ------------------ |
| openrouter-google_gemma-3-12b-it_free                   | 76.3              | 2.308029       | 0.914              |
| openrouter-google_gemma-3-27b-it_free                   | 22.58             | 2.308029       | 0.241              |
| openrouter-google_gemma-3-4b-it_free                    | 59.08             | 2.308029       | 0.534              |
| openrouter-nvidia_nemotron-nano-12b-v2-vl_free          | 68.01             | 2.308029       | 0.776              |
| openrouter-qwen_qwen-2.5-vl-7b-instruct_free            | 70.9              | 2.308029       | 0.707              |

## Interpretation

The results indicate significant performance differences between the models. The `openrouter-google_gemma-3-12b-it_free` model achieved the highest app quality score (76.3) and a high schema strict rate (0.914). The `openrouter-google_gemma-3-27b-it_free` model performed poorly, with a low app quality score (22.58) and schema strict rate (0.241), and a high number of missing predictions.

Based on the bootstrap comparisons, `openrouter-google_gemma-3-12b-it_free` is significantly better than `openrouter-google_gemma-3-27b-it_free`, `openrouter-google_gemma-3-4b-it_free`, and `openrouter-nvidia_nemotron-nano-12b-v2-vl_free`. The difference between `openrouter-google_gemma-3-12b-it_free` and `openrouter-qwen_qwen-2.5-vl-7b-instruct_free` is not statistically significant (p-value = 0.052).

The cost for all models was the same (2.308029 USD), representing the ground truth cost. Therefore, the primary tradeoff is between accuracy (app quality score) and schema adherence (schema strict rate).

## Limitations

The primary limitation is the use of **silver** ground truth. This means the absolute accuracy of the models cannot be definitively determined. The results are useful for relative comparisons, but a gold-standard evaluation with human-verified ground truth is needed for a more accurate assessment. The dataset size is also limited, and the geographic bias towards Thailand may affect the generalizability of the results. The models were evaluated with a fixed temperature and seed, which may not represent their optimal performance under different settings. The maximum output caps are not available.

## Recommendations

Based on this silver-standard evaluation, the `openrouter-google_gemma-3-12b-it_free` model appears to be the most promising for the ArtistCalendar application, given its high app quality score and schema strict rate. However, a gold-standard evaluation is strongly recommended to validate these findings and obtain a more accurate assessment of the models' performance. Further investigation into the causes of the poor performance of the `openrouter-google_gemma-3-27b-it_free` model is also warranted.
